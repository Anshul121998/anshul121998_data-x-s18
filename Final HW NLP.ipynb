{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"prideNprejudice.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It is a truth universally acknowledged, that a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"My dear Mr. Bennet,\" said his lady to him one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bennet replied that he had not.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"But it is,\" returned she; \"for Mrs. Long has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bennet made no answer.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences\n",
       "0  It is a truth universally acknowledged, that a...\n",
       "1  \"My dear Mr. Bennet,\" said his lady to him one...\n",
       "2                    Bennet replied that he had not.\n",
       "3  \"But it is,\" returned she; \"for Mrs. Long has ...\n",
       "4                             Bennet made no answer."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From NLTK notebook shown\n",
    "def sentence_cleaner(sentence):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "    \n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    \n",
    "    #1. Remove HTML tags\n",
    "    sentence = bs.BeautifulSoup(sentence).text\n",
    "    warnings.filterwarnings('ignore')\n",
    "    #2. Use regex to find emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', sentence)\n",
    "    \n",
    "    #3. Remove punctuation\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \",sentence)\n",
    "    \n",
    "    #4. Tokenize into words (all lower case)\n",
    "    sentence = sentence.lower().split()\n",
    "    \n",
    "    #5. Remove stopwords\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    sentence = [w for w in sentence if not w in eng_stopwords]\n",
    "    \n",
    "    #6. Join the review to one sentence\n",
    "    sentence = ' '.join(sentence+emoticons)\n",
    "    # add emoticons to the end\n",
    "\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentence_clean_original = []\n",
    "\n",
    "for i in range(0,size):\n",
    "    sentence_clean_original.append(sentence_cleaner(df['sentences'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'truth universally acknowledged single man possessionof good fortune must want wife however little known feelings views man may hisfirst entering neighbourhood truth well fixed mindsof surrounding families considered rightful propertyof one daughters'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_clean_original[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range (0,len(sentence_clean_original)):\n",
    "    mystr = sentence_clean_original[i]\n",
    "    wordList = re.sub(\"[^\\w]\", \" \",  mystr).split()\n",
    "    l = l + wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'truth'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-24 23:53:21,047 : INFO : collecting all words and their counts\n",
      "2018-04-24 23:53:21,072 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-04-24 23:53:21,115 : INFO : collected 11632 word types from a corpus of 55184 raw words and 1 sentences\n",
      "2018-04-24 23:53:21,121 : INFO : Loading a fresh vocabulary\n",
      "2018-04-24 23:53:21,146 : INFO : min_count=5 retains 1817 unique words (15% of original 11632, drops 9815)\n",
      "2018-04-24 23:53:21,149 : INFO : min_count=5 leaves 41518 word corpus (75% of original 55184, drops 13666)\n",
      "2018-04-24 23:53:21,278 : INFO : deleting the raw counts dictionary of 11632 items\n",
      "2018-04-24 23:53:21,283 : INFO : sample=0.001 downsamples 53 most-common words\n",
      "2018-04-24 23:53:21,287 : INFO : downsampling leaves estimated 37407 word corpus (90.1% of prior 41518)\n",
      "2018-04-24 23:53:21,328 : INFO : estimated required memory for 1817 words and 55184 dimensions: 803063124 bytes\n",
      "2018-04-24 23:53:21,331 : INFO : resetting layer weights\n",
      "2018-04-24 23:53:24,597 : INFO : training model with 3 workers on 1817 vocabulary and 55184 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-04-24 23:53:24,623 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:53:24,629 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:53:48,577 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 419 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:53:48,622 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:53:48,628 : INFO : EPOCH - 1 : training on 55184 raw words (10000 effective words) took 24.0s, 416 effective words/s\n",
      "2018-04-24 23:53:53,189 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:53:53,205 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:54:16,864 : INFO : EPOCH 2 - PROGRESS: at 100.00% examples, 419 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:54:16,868 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:54:16,871 : INFO : EPOCH - 2 : training on 55184 raw words (10000 effective words) took 23.9s, 419 effective words/s\n",
      "2018-04-24 23:54:17,039 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:54:17,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:54:35,578 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 539 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:54:35,780 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:54:35,784 : INFO : EPOCH - 3 : training on 55184 raw words (10000 effective words) took 18.8s, 533 effective words/s\n",
      "2018-04-24 23:54:35,991 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:54:36,005 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:54:56,007 : INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 499 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:54:56,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:54:56,014 : INFO : EPOCH - 4 : training on 55184 raw words (10000 effective words) took 20.0s, 499 effective words/s\n",
      "2018-04-24 23:54:56,028 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:54:56,034 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:55:11,304 : INFO : EPOCH 5 - PROGRESS: at 100.00% examples, 654 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:55:11,307 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:55:11,311 : INFO : EPOCH - 5 : training on 55184 raw words (10000 effective words) took 15.3s, 654 effective words/s\n",
      "2018-04-24 23:55:11,327 : INFO : training on a 275920 raw words (50000 effective words) took 106.7s, 468 effective words/s\n",
      "2018-04-24 23:55:11,331 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-24 23:55:12,754 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-04-24 23:55:12,759 : INFO : training model with 3 workers on 1817 vocabulary and 55184 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-04-24 23:55:12,821 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:55:12,829 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:55:30,311 : INFO : EPOCH 1 - PROGRESS: at 0.00% examples, 571 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:55:30,314 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:55:30,321 : INFO : EPOCH - 1 : training on 55184 raw words (10000 effective words) took 17.5s, 571 effective words/s\n",
      "2018-04-24 23:55:30,325 : WARNING : EPOCH - 1 : supplied example count (1) did not equal expected count (55184)\n",
      "2018-04-24 23:55:30,344 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:55:30,350 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:55:46,588 : INFO : EPOCH 2 - PROGRESS: at 0.00% examples, 615 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:55:46,591 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:55:46,596 : INFO : EPOCH - 2 : training on 55184 raw words (10000 effective words) took 16.3s, 615 effective words/s\n",
      "2018-04-24 23:55:46,600 : WARNING : EPOCH - 2 : supplied example count (1) did not equal expected count (55184)\n",
      "2018-04-24 23:55:46,627 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:55:46,632 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:56:06,281 : INFO : EPOCH 3 - PROGRESS: at 0.00% examples, 508 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:56:06,284 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:56:06,288 : INFO : EPOCH - 3 : training on 55184 raw words (10000 effective words) took 19.7s, 508 effective words/s\n",
      "2018-04-24 23:56:06,293 : WARNING : EPOCH - 3 : supplied example count (1) did not equal expected count (55184)\n",
      "2018-04-24 23:56:06,311 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:56:06,317 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:56:26,506 : INFO : EPOCH 4 - PROGRESS: at 0.00% examples, 495 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:56:26,509 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:56:26,513 : INFO : EPOCH - 4 : training on 55184 raw words (10000 effective words) took 20.2s, 495 effective words/s\n",
      "2018-04-24 23:56:26,517 : WARNING : EPOCH - 4 : supplied example count (1) did not equal expected count (55184)\n",
      "2018-04-24 23:56:26,537 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:56:26,543 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:56:45,986 : INFO : EPOCH 5 - PROGRESS: at 0.00% examples, 514 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:56:45,988 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:56:45,992 : INFO : EPOCH - 5 : training on 55184 raw words (10000 effective words) took 19.5s, 514 effective words/s\n",
      "2018-04-24 23:56:45,995 : WARNING : EPOCH - 5 : supplied example count (1) did not equal expected count (55184)\n",
      "2018-04-24 23:56:46,009 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:56:46,015 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:56:56,671 : INFO : EPOCH 6 - PROGRESS: at 0.00% examples, 937 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:56:56,674 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:56:56,677 : INFO : EPOCH - 6 : training on 55184 raw words (10000 effective words) took 10.7s, 937 effective words/s\n",
      "2018-04-24 23:56:56,680 : WARNING : EPOCH - 6 : supplied example count (1) did not equal expected count (55184)\n",
      "2018-04-24 23:56:56,696 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:56:56,702 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:57:10,269 : INFO : EPOCH 7 - PROGRESS: at 0.00% examples, 736 words/s, in_qsize 0, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-24 23:57:10,272 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:57:10,276 : INFO : EPOCH - 7 : training on 55184 raw words (10000 effective words) took 13.6s, 736 effective words/s\n",
      "2018-04-24 23:57:10,281 : WARNING : EPOCH - 7 : supplied example count (1) did not equal expected count (55184)\n",
      "2018-04-24 23:57:10,299 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:57:10,305 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:57:24,443 : INFO : EPOCH 8 - PROGRESS: at 0.00% examples, 707 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:57:24,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:57:24,449 : INFO : EPOCH - 8 : training on 55184 raw words (10000 effective words) took 14.2s, 706 effective words/s\n",
      "2018-04-24 23:57:24,453 : WARNING : EPOCH - 8 : supplied example count (1) did not equal expected count (55184)\n",
      "2018-04-24 23:57:24,472 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:57:24,476 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:57:37,871 : INFO : EPOCH 9 - PROGRESS: at 0.00% examples, 746 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:57:37,874 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:57:37,879 : INFO : EPOCH - 9 : training on 55184 raw words (10000 effective words) took 13.4s, 745 effective words/s\n",
      "2018-04-24 23:57:37,883 : WARNING : EPOCH - 9 : supplied example count (1) did not equal expected count (55184)\n",
      "2018-04-24 23:57:37,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-24 23:57:37,909 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-24 23:57:58,348 : INFO : EPOCH 10 - PROGRESS: at 0.00% examples, 489 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-24 23:57:58,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-24 23:57:58,354 : INFO : EPOCH - 10 : training on 55184 raw words (10000 effective words) took 20.5s, 489 effective words/s\n",
      "2018-04-24 23:57:58,357 : WARNING : EPOCH - 10 : supplied example count (1) did not equal expected count (55184)\n",
      "2018-04-24 23:57:58,361 : INFO : training on a 551840 raw words (100000 effective words) took 165.6s, 604 effective words/s\n",
      "2018-04-24 23:57:58,365 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100000, 551840)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec([l], size=len(l))\n",
    "model.train([l],total_examples=len(l),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0089632  -0.0092427  -0.03871374 ..., -0.01692482  0.00929901\n",
      " -0.00245467]\n"
     ]
    }
   ],
   "source": [
    "print(model['girls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999937437168585"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('elizabeth','girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-24 23:59:47,374 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rest', 1.0000008344650269),\n",
       " ('making', 1.0000007152557373),\n",
       " ('consequence', 1.0000004768371582),\n",
       " ('temper', 1.0000003576278687),\n",
       " ('avoid', 1.000000238418579),\n",
       " ('life', 1.000000238418579)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar([\"girl\"],topn = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-25 00:03:10,486 : WARNING : vectors for words {'universally'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'girls'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match([\"girls\",\"elizabeth\",\"universally\",\"lady\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999985882839926"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('bennet','long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
